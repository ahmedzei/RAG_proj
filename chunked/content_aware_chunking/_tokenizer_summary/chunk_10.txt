So to get the best of
both worlds, transformers models use a hybrid between word-level and character-level tokenization called subword
tokenization.
Subword tokenization

Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller
subwords, but rare words should be decomposed into meaningful subwords. For instance "annoyingly" might be
considered a rare word and could be decomposed into "annoying" and "ly".