Applying them on our example, spaCy and Moses would output something like:
["Do", "n't", "you", "love", "ðŸ¤—", "Transformers", "?", "We", "sure", "do", "."]
As can be seen space and punctuation tokenization, as well as rule-based tokenization, is used here. Space and
punctuation tokenization and rule-based tokenization are both examples of word tokenization, which is loosely defined
as splitting sentences into words.