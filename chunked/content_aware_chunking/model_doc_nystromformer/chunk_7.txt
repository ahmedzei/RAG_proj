We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard
sequence length, and find that our Nystr√∂mformer performs comparably, or in a few cases, even slightly better, than
standard self-attention.