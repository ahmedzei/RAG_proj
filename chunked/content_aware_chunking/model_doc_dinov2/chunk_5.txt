In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.
This model was contributed by nielsr.
The original code can be found here.
Usage tips
The model can be traced using torch.jit.trace which leverages JIT compilation to optimize the model making it faster to run.