This section only provides a brief and simple example.

ONNX Runtime (ORT) is a model accelerator that runs inference on CPUs by default. ORT is supported by ðŸ¤— Optimum which can be used in ðŸ¤— Transformers, without making too many changes to your code.