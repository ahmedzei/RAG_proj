Mixtral
Overview
Mixtral-8x7B is Mistral AI's second Large Language Model (LLM). 
The Mixtral model was proposed by the Mistral AI team.
It was introduced in the Mixtral of Experts blogpost with the following introduction:
Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts models (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference.