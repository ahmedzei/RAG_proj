To avoid this, you should set the environment variable PYTORCH_ENABLE_MPS_FALLBACK=1 to use the CPU kernels instead (you'll still see a UserWarning).

If you run into any other errors, please open an issue in the PyTorch repository because the [Trainer] only integrates the MPS backend.

With the mps device set, you can:

train larger networks or batch sizes locally
reduce data retrieval latency because the GPU's unified memory architecture allows direct access to the full memory store
reduce costs because you don't need to train on cloud-based GPUs or add additional local GPUs

Get started by making sure you have PyTorch installed.