This only works, however, if the string you pass to
  [fairseq.encode] starts with a space.
[~generation.GenerationMixin.generate] should be used for conditional generation tasks like
  summarization, see the example in that docstrings.
Models that load the facebook/bart-large-cnn weights will not have a mask_token_id, or be able to perform
  mask-filling tasks.

Mask Filling
The facebook/bart-base and facebook/bart-large checkpoints can be used to fill multi-token masks.
thon
from transformers import BartForConditionalGeneration, BartTokenizer
model = BartForConditionalGeneration.from_pretrained("facebook/bart-large", forced_bos_token_id=0)
tok = BartTokenizer.from_pretrained("facebook/bart-large")
example_english_phrase = "UN Chief Says There Is No  in Syria"
batch = tok(example_english_phrase, return_tensors="pt")
generated_ids = model.generate(batch["input_ids"])
assert tok.batch_decode(generated_ids, skip_special_tokens=True) == [
    "UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria"
]

Resources
A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with BART.