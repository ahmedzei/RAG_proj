`torch.float16``)
To load and run a model using Flash Attention 2, refer to the snippet below:
thon

import torch
from transformers import PhiForCausalLM, AutoTokenizer
define the model and tokenizer and push the model and tokens to the GPU.
model = PhiForCausalLM.from_pretrained("microsoft/phi-1_5", torch_dtype=torch.float16, attn_implementation="flash_attention_2").to("cuda")
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1_5")
feel free to change the prompt to your liking.
prompt = "If I were an AI that had just achieved"
apply the tokenizer.
tokens = tokenizer(prompt, return_tensors="pt").to("cuda")
use the model to generate new tokens.
generated_output = model.generate(**tokens, use_cache=True, max_new_tokens=10)
tokenizer.batch_decode(generated_output)[0]
'If I were an AI that had just achieved a breakthrough in machine learning, I would be thrilled'

Expected speedups
Below is an expected speedup diagram that compares pure inference time between the native implementation in transformers using microsoft/phi-1 checkpoint and the Flash Attention 2 version of the model using a sequence length of 2048.

PhiConfig
[[autodoc]] PhiConfig

PhiModel
[[autodoc]] PhiModel
    - forward
PhiForCausalLM
[[autodoc]] PhiForCausalLM
    - forward
    - generate
PhiForSequenceClassification
[[autodoc]] PhiForSequenceClassification
    - forward
PhiForTokenClassification
[[autodoc]] PhiForTokenClassification
    - forward

.