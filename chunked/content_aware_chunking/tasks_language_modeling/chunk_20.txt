Note that Transformers models all have a default task-relevant loss function, so you don't need to specify one unless you want to:

import tensorflow as tf
model.compile(optimizer=optimizer)  # No loss argument!

This can be done by specifying where to push your model and tokenizer in the [~transformers.PushToHubCallback]:

from transformers.keras_callbacks import PushToHubCallback
callback = PushToHubCallback(
     output_dir="my_awesome_eli5_clm-model",
     tokenizer=tokenizer,
 )

Finally, you're ready to start training your model! Call fit with your training and validation datasets, the number of epochs, and your callback to finetune the model:

model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])

Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!

For a more in-depth example of how to finetune a model for causal language modeling, take a look at the corresponding
PyTorch notebook
or TensorFlow notebook.

Inference
Great, now that you've finetuned a model, you can use it for inference!
Come up with a prompt you'd like to generate text from:

prompt = "Somatic hypermutation allows the immune system to"

The simplest way to try out your finetuned model for inference is to use it in a [pipeline].