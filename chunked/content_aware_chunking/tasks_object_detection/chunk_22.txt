However, images
should stay as they are.
The evaluation step requires a bit of work, but it can be split in three major steps.
First, prepare the cppe5["test"] set: format the annotations and save the data to disk.

import json
format annotations the same as for training, no need for data augmentation
def val_formatted_anns(image_id, objects):
     annotations = []
     for i in range(0, len(objects["id"])):
         new_ann = {
             "id": objects["id"][i],
             "category_id": objects["category"][i],
             "iscrowd": 0,
             "image_id": image_id,
             "area": objects["area"][i],
             "bbox": objects["bbox"][i],
         }
         annotations.append(new_ann)

     return annotations

Save images and annotations into the files torchvision.datasets.CocoDetection expects
def save_cppe5_annotation_file_images(cppe5):
     output_json = {}
     path_output_cppe5 = f"{os.getcwd()}/cppe5/"

     if not os.path.exists(path_output_cppe5):
         os.makedirs(path_output_cppe5)
     path_anno = os.path.join(path_output_cppe5, "cppe5_ann.json")
     categories_json = [{"supercategory": "none", "id": id, "name": id2label[id]} for id in id2label]
     output_json["images"] = []
     output_json["annotations"] = []
     for example in cppe5:
         ann = val_formatted_anns(example["image_id"], example["objects"])
         output_json["images"].append(
             {
                 "id": example["image_id"],
                 "width": example["image"].width,
                 "height": example["image"].height,
                 "file_name": f"{example['image_id']}.png",
             }
         )
         output_json["annotations"].extend(ann)
     output_json["categories"] = categories_json
     with open(path_anno, "w") as file:
         json.dump(output_json, file, ensure_ascii=False, indent=4)
     for im, img_id in zip(cppe5["image"], cppe5["image_id"]):
         path_img = os.path.join(path_output_cppe5, f"{img_id}.png")
         im.save(path_img)
     return path_output_cppe5, path_anno

Next, prepare an instance of a CocoDetection class that can be used with cocoevaluator.

import torchvision
class CocoDetection(torchvision.datasets.CocoDetection):
     def init(self, img_folder, image_processor, ann_file):
         super().init(img_folder, ann_file)
         self.image_processor = image_processor

     def getitem(self, idx):
         # read in PIL image and target in COCO format
         img, target = super(CocoDetection, self).getitem(idx)
         # preprocess image and target: converting target to DETR format,
         # resizing + normalization of both image and target)
         image_id = self.ids[idx]
         target = {"image_id": image_id, "annotations": target}
         encoding = self.image_processor(images=img, annotations=target, return_tensors="pt")
         pixel_values = encoding["pixel_values"].squeeze()  # remove batch dimension
         target = encoding["labels"][0]  # remove batch dimension
         return {"pixel_values": pixel_values, "labels": target}

im_processor = AutoImageProcessor.from_pretrained("devonho/detr-resnet-50_finetuned_cppe5")
path_output_cppe5, path_anno = save_cppe5_annotation_file_images(cppe5["test"])
test_ds_coco_format = CocoDetection(path_output_cppe5, im_processor, path_anno)

Finally, load the metrics and run the evaluation.

import evaluate
from tqdm import tqdm
model = AutoModelForObjectDetection.from_pretrained("devonho/detr-resnet-50_finetuned_cppe5")
module = evaluate.load("ybelkada/cocoevaluate", coco=test_ds_coco_format.coco)
val_dataloader = torch.utils.data.DataLoader(
     test_ds_coco_format, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn
 )
with torch.no_grad():
     for idx, batch in enumerate(tqdm(val_dataloader)):
         pixel_values = batch["pixel_values"]
         pixel_mask = batch["pixel_mask"]

         labels = [
             {k: v for k, v in t.items()} for t in batch["labels"]
         ]  # these are in DETR format, resized + normalized
         # forward pass
         outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)
         orig_target_sizes = torch.stack([target["orig_size"] for target in labels], dim=0)
         results = im_processor.post_process(outputs, orig_target_sizes)  # convert outputs of model to Pascal VOC format (xmin, ymin, xmax, ymax)
         module.add(prediction=results, reference=labels)
         del batch

results = module.compute()
print(results)
Accumulating evaluation results
DONE (t=0.08s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.352
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.681
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.292
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.168
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.429
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.274
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.484
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.501
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.191
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.323
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.590
``
These results can be further improved by adjusting the hyperparameters in [~transformers.TrainingArguments`].