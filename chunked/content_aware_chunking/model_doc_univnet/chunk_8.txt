See this issue for more details.

Usage Example:
thon
import torch
from scipy.io.wavfile import write
from datasets import Audio, load_dataset
from transformers import UnivNetFeatureExtractor, UnivNetModel
model_id_or_path = "dg845/univnet-dev"
model = UnivNetModel.from_pretrained(model_id_or_path)
feature_extractor = UnivNetFeatureExtractor.from_pretrained(model_id_or_path)
ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
Resample the audio to the model and feature extractor's sampling rate.
ds = ds.cast_column("audio", Audio(sampling_rate=feature_extractor.sampling_rate))
Pad the end of the converted waveforms to reduce artifacts at the end of the output audio samples.
inputs = feature_extractor(
    ds[0]["audio"]["array"], sampling_rate=ds[0]["audio"]["sampling_rate"], pad_end=True, return_tensors="pt"
)
with torch.no_grad():
    audio = model(**inputs)
Remove the extra padding at the end of the output.
audio = feature_extractor.batch_decode(**audio)[0]
Convert to wav file
write("sample_audio.wav", feature_extractor.sampling_rate, audio)

This model was contributed by dg845.
To the best of my knowledge, there is no official code release, but an unofficial implementation can be found at maum-ai/univnet with pretrained checkpoints here.
UnivNetConfig
[[autodoc]] UnivNetConfig
UnivNetFeatureExtractor
[[autodoc]] UnivNetFeatureExtractor
    - call
UnivNetModel
[[autodoc]] UnivNetModel
    - forward.