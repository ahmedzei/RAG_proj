LayoutLMv2 on the other hand normalizes the images internally and expects the channels in BGR format.
text is tokenized using byte-pair encoding (BPE), as opposed to WordPiece.
  Due to these differences in data preprocessing, one can use [LayoutLMv3Processor] which internally combines a [LayoutLMv3ImageProcessor] (for the image modality) and a [LayoutLMv3Tokenizer]/[LayoutLMv3TokenizerFast] (for the text modality) to prepare all data for the model.

Regarding usage of [LayoutLMv3Processor], we refer to the usage guide of its predecessor.

Resources
A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with LayoutLMv3.