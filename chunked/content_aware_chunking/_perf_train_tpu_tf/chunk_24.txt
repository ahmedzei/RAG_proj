Take a look at our TPU example notebook to see this in action!
Summary
There was a lot in here, so letâ€™s summarize with a quick checklist you can follow when you want to get your model ready for TPU training:

Make sure your code follows the three rules of XLA
Compile your model with jit_compile=True on CPU/GPU and confirm that you can train it with XLA
Either load your dataset into memory or use a TPU-compatible dataset loading approach (see notebook)
Migrate your code either to Colab (with accelerator set to â€œTPUâ€) or a TPU VM on Google Cloud
Add TPU initializer code (see notebook)
Create your TPUStrategy and make sure dataset loading and model creation are inside the strategy.scope() (see notebook)
Donâ€™t forget to take jit_compile=True out again when you move to TPU!
ğŸ™ğŸ™ğŸ™ğŸ¥ºğŸ¥ºğŸ¥º
Call model.fit()
You did it!
.