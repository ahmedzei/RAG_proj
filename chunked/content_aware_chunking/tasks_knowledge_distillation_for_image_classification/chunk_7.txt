Thus, in the context of knowledge distillation, KL divergence is useful.
thon
from transformers import TrainingArguments, Trainer
import torch
import torch.nn as nn
import torch.nn.functional as F
class ImageDistilTrainer(Trainer):
    def init(self, teacher_model=None, student_model=None, temperature=None, lambda_param=None,  args, kwargs):
        super().init(model=student_model, args, **kwargs)
        self.teacher = teacher_model
        self.student = student_model
        self.loss_function = nn.KLDivLoss(reduction="batchmean")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.teacher.to(device)
        self.teacher.eval()
        self.temperature = temperature
        self.lambda_param = lambda_param
def compute_loss(self, student, inputs, return_outputs=False):
    student_output = self.student(**inputs)

    with torch.no_grad():
      teacher_output = self.teacher(**inputs)

    # Compute soft targets for teacher and student
    soft_teacher = F.softmax(teacher_output.logits / self.temperature, dim=-1)
    soft_student = F.log_softmax(student_output.logits / self.temperature, dim=-1)

    # Compute the loss
    distillation_loss = self.loss_function(soft_student, soft_teacher) * (self.temperature ** 2)

    # Compute the true label loss
    student_target_loss = student_output.loss

    # Calculate final loss
    loss = (1.