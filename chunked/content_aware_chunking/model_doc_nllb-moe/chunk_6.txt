Moreover, if a token is not routed to any expert, SwitchTransformers still adds its unmodified hidden 
states (kind of like a residual connection) while they are masked in NLLB's top-2 routing mechanism. 
Generating with NLLB-MoE
The available checkpoints require around 350GB of storage. Make sure to use accelerate if you do not have enough RAM on your machine.
While generating the target text set the forced_bos_token_id to the target language id.