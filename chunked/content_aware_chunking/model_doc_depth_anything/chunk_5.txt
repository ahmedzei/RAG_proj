Taken from the original paper.
This model was contributed by nielsr.
The original code can be found here.
Usage example
There are 2 main ways to use Depth Anything: either using the pipeline API, which abstracts away all the complexity for you, or by using the DepthAnythingForDepthEstimation class yourself.
Pipeline API
The pipeline allows to use the model in a few lines of code:
thon

from transformers import pipeline
from PIL import Image
import requests
load pipe
pipe = pipeline(task="depth-estimation", model="LiheYoung/depth-anything-small-hf")
load image
url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)
inference
depth = pipe(image)["depth"]

Using the model yourself
If you want to do the pre- and postprocessing yourself, here's how to do that:
thon

from transformers import AutoImageProcessor, AutoModelForDepthEstimation
import torch
import numpy as np
from PIL import Image
import requests
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
image_processor = AutoImageProcessor.from_pretrained("LiheYoung/depth-anything-small-hf")
model = AutoModelForDepthEstimation.from_pretrained("LiheYoung/depth-anything-small-hf")
prepare image for the model
inputs = image_processor(images=image, return_tensors="pt")
with torch.no_grad():
     outputs = model(**inputs)
     predicted_depth = outputs.predicted_depth
interpolate to original size
prediction = torch.nn.functional.interpolate(
     predicted_depth.unsqueeze(1),
     size=image.size[::-1],
     mode="bicubic",
     align_corners=False,
 )
visualize the prediction
output = prediction.squeeze().cpu().numpy()
formatted = (output * 255 / np.max(output)).astype("uint8")
depth = Image.fromarray(formatted)

Resources
A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with Depth Anything.

Monocular depth estimation task guide
A notebook showcasing inference with [DepthAnythingForDepthEstimation] can be found here.