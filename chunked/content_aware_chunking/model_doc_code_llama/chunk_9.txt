You should be using the tokenizer.fill_token where you want your input to be filled.
- The model conversion script is the same as for the Llama2 family:
Here is a sample usage:

python src/transformers/models/llama/convert_llama_weights_to_hf.py \
    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path
Note that executing the script requires enough CPU RAM to host the whole model in float16 precision (even if the biggest versions
come in several checkpoints they each contain a part of each weight of the model, so we need to load them all in RAM).
After conversion, the model and tokenizer can be loaded via:
thon

from transformers import LlamaForCausalLM, CodeLlamaTokenizer
tokenizer = CodeLlamaTokenizer.from_pretrained("codellama/CodeLlama-7b-hf")
model = LlamaForCausalLM.from_pretrained("codellama/CodeLlama-7b-hf")
PROMPT = '''def remove_non_ascii(s: str) -> str:
    """ 
    return result
'''
input_ids = tokenizer(PROMPT, return_tensors="pt")["input_ids"]
generated_ids = model.generate(input_ids, max_new_tokens=128)
filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]
print(PROMPT.replace("", filling))
def remove_non_ascii(s: str) -> str:
    """ Remove non-ASCII characters from a string.

Args:
    s: The string to remove non-ASCII characters from.

Returns:
    The string with non-ASCII characters removed.
"""
result = ""
for c in s:
    if ord(c) < 128:
        result += c
return result

If you only want the infilled part:
thon

from transformers import pipeline
import torch
generator = pipeline("text-generation",model="codellama/CodeLlama-7b-hf",torch_dtype=torch.float16, device_map="auto")
generator('def remove_non_ascii(s: str) -> str:\n    """ \n    return result', max_new_tokens = 128, return_type = 1)

Under the hood, the tokenizer automatically splits by <FILL_ME> to create a formatted input string that follows the original training pattern.