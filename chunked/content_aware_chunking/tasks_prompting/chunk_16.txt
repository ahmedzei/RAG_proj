Once again, here's how 
you can write a basic prompt to instruct a model to translate a piece of text from English to Italian: 
thon

torch.manual_seed(2) # doctest: +IGNORE_RESULT
prompt = """Translate the English text to Italian.
 Text: Sometimes, I've believed as many as six impossible things before breakfast.
 Translation:
 """
sequences = pipe(
     prompt,
     max_new_tokens=20,
     do_sample=True,
     top_k=10,
     return_full_text = False,
 )
for seq in sequences:
     print(f"{seq['generated_text']}")
A volte, ho creduto a sei impossibili cose prima di colazione.

Here we've added a do_sample=True and top_k=10 to allow the model to be a bit more flexible when generating output.
Text summarization
Similar to the translation, text summarization is another generative task where the output heavily relies on the input, 
and encoder-decoder models can be a better choice.