In the
following example, we load an audio file using the ðŸ¤— Datasets library, which can be pip installed through the command
below:

pip install --upgrade pip
pip install datasets[audio]
thon

from transformers import AutoProcessor, MusicgenForConditionalGeneration
from datasets import load_dataset
processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")
dataset = load_dataset("sanchit-gandhi/gtzan", split="train", streaming=True)
sample = next(iter(dataset))["audio"]
take the first half of the audio sample
sample["array"] = sample["array"][: len(sample["array"]) // 2]
inputs = processor(
     audio=sample["array"],
     sampling_rate=sample["sampling_rate"],
     text=["80s blues track with groovy saxophone"],
     padding=True,
     return_tensors="pt",
 )
audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)

For batched audio-prompted generation, the generated audio_values can be post-processed to remove padding by using the
[MusicgenProcessor] class:
thon

from transformers import AutoProcessor, MusicgenForConditionalGeneration
from datasets import load_dataset
processor = AutoProcessor.from_pretrained("facebook/musicgen-small")
model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")
dataset = load_dataset("sanchit-gandhi/gtzan", split="train", streaming=True)
sample = next(iter(dataset))["audio"]
take the first quarter of the audio sample
sample_1 = sample["array"][: len(sample["array"]) // 4]
take the first half of the audio sample
sample_2 = sample["array"][: len(sample["array"]) // 2]
inputs = processor(
     audio=[sample_1, sample_2],
     sampling_rate=sample["sampling_rate"],
     text=["80s blues track with groovy saxophone", "90s rock song with loud guitars and heavy drums"],
     padding=True,
     return_tensors="pt",
 )
audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)
post-process to remove padding from the batched audio
audio_values = processor.batch_decode(audio_values, padding_mask=inputs.padding_mask)

Generation Configuration
The default parameters that control the generation process, such as sampling, guidance scale and number of generated 
tokens, can be found in the model's generation config, and updated as desired:
thon

from transformers import MusicgenForConditionalGeneration
model = MusicgenForConditionalGeneration.from_pretrained("facebook/musicgen-small")
inspect the default generation config
model.generation_config
increase the guidance scale to 4.0
model.generation_config.guidance_scale = 4.0
decrease the max length to 256 tokens
model.generation_config.max_length = 256

Note that any arguments passed to the generate method will supersede those in the generation config, so setting 
do_sample=False in the call to generate will supersede the setting of model.generation_config.do_sample in the 
generation config.
Model Structure
The MusicGen model can be de-composed into three distinct stages:
1.