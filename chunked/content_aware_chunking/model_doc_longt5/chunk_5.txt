Furthermore, it uses a different pre-training objective
inspired by the pre-training of [PegasusForConditionalGeneration].
LongT5 model is designed to work efficiently and very well on long-range sequence-to-sequence tasks where the
input sequence exceeds commonly used 512 tokens.