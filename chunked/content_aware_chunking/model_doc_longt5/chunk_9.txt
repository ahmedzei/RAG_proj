As a consequence, TGlobal attention introduces
a few new parameters -- global relative position biases and a layer normalization for global token's embedding.
The complexity of this mechanism is O(l(r + l/k)).
An example showing how to evaluate a fine-tuned LongT5 model on the pubmed dataset is below.

thon

import evaluate
from datasets import load_dataset
from transformers import AutoTokenizer, LongT5ForConditionalGeneration
dataset = load_dataset("scientific_papers", "pubmed", split="validation")
model = (
     LongT5ForConditionalGeneration.from_pretrained("Stancld/longt5-tglobal-large-16384-pubmed-3k_steps")
     .to("cuda")
     .half()
 )
tokenizer = AutoTokenizer.from_pretrained("Stancld/longt5-tglobal-large-16384-pubmed-3k_steps")
def generate_answers(batch):
     inputs_dict = tokenizer(
         batch["article"], max_length=16384, padding="max_length", truncation=True, return_tensors="pt"
     )
     input_ids = inputs_dict.input_ids.to("cuda")
     attention_mask = inputs_dict.attention_mask.to("cuda")
     output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=512, num_beams=2)
     batch["predicted_abstract"] = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
     return batch
result = dataset.map(generate_answer, batched=True, batch_size=2)
rouge = evaluate.load("rouge")
rouge.compute(predictions=result["predicted_abstract"], references=result["abstract"])

Resources

Translation task guide
Summarization task guide

LongT5Config
[[autodoc]] LongT5Config

LongT5Model
[[autodoc]] LongT5Model
    - forward
LongT5ForConditionalGeneration
[[autodoc]] LongT5ForConditionalGeneration
    - forward
LongT5EncoderModel
[[autodoc]] LongT5EncoderModel
    - forward

FlaxLongT5Model
[[autodoc]] FlaxLongT5Model
    - call
    - encode
    - decode
FlaxLongT5ForConditionalGeneration
[[autodoc]] FlaxLongT5ForConditionalGeneration
    - call
    - encode
    - decode

.