The original code can be found here.
Usage tips and examples
BridgeTower consists of a visual encoder, a textual encoder and cross-modal encoder with multiple lightweight bridge layers.
The goal of this approach was to build a bridge between each uni-modal encoder and the cross-modal encoder to enable comprehensive and detailed interaction at each layer of the cross-modal encoder.
In principle, one can apply any visual, textual or cross-modal encoder in the proposed architecture.
The [BridgeTowerProcessor] wraps [RobertaTokenizer] and [BridgeTowerImageProcessor] into a single instance to both
encode the text and prepare the images respectively.
The following example shows how to run contrastive learning using [BridgeTowerProcessor] and [BridgeTowerForContrastiveLearning].
thon

from transformers import BridgeTowerProcessor, BridgeTowerForContrastiveLearning
import requests
from PIL import Image
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
texts = ["An image of two cats chilling on a couch", "A football player scoring a goal"]
processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-large-itm-mlm-itc")
model = BridgeTowerForContrastiveLearning.from_pretrained("BridgeTower/bridgetower-large-itm-mlm-itc")
forward pass
scores = dict()
for text in texts:
     # prepare inputs
     encoding = processor(image, text, return_tensors="pt")
     outputs = model(**encoding)
     scores[text] = outputs

The following example shows how to run image-text retrieval using [BridgeTowerProcessor] and [BridgeTowerForImageAndTextRetrieval].
thon

from transformers import BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval
import requests
from PIL import Image
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
texts = ["An image of two cats chilling on a couch", "A football player scoring a goal"]
processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")
model = BridgeTowerForImageAndTextRetrieval.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")
forward pass
scores = dict()
for text in texts:
     # prepare inputs
     encoding = processor(image, text, return_tensors="pt")
     outputs = model(**encoding)
     scores[text] = outputs.logits[0, 1].item()

The following example shows how to run masked language modeling using [BridgeTowerProcessor] and [BridgeTowerForMaskedLM].
thon

from transformers import BridgeTowerProcessor, BridgeTowerForMaskedLM
from PIL import Image
import requests
url = "http://images.cocodataset.org/val2017/000000360943.jpg"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
text = "a  looking out of the window"
processor = BridgeTowerProcessor.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")
model = BridgeTowerForMaskedLM.from_pretrained("BridgeTower/bridgetower-base-itm-mlm")
prepare inputs
encoding = processor(image, text, return_tensors="pt")
forward pass
outputs = model(**encoding)
results = processor.decode(outputs.logits.argmax(dim=-1).squeeze(0).tolist())
print(results)
.a cat looking out of the window.

Tips:

This implementation of BridgeTower uses [RobertaTokenizer] to generate text embeddings and OpenAI's CLIP/ViT model to compute visual embeddings.
Checkpoints for pre-trained bridgeTower-base and bridgetower masked language modeling and image text matching are released.
Please refer to Table 5 for BridgeTower's performance on Image Retrieval and other down stream tasks.
The PyTorch version of this model is only available in torch 1.10 and higher.

BridgeTowerConfig
[[autodoc]] BridgeTowerConfig
BridgeTowerTextConfig
[[autodoc]] BridgeTowerTextConfig
BridgeTowerVisionConfig
[[autodoc]] BridgeTowerVisionConfig
BridgeTowerImageProcessor
[[autodoc]] BridgeTowerImageProcessor
    - preprocess
BridgeTowerProcessor
[[autodoc]] BridgeTowerProcessor
    - call
BridgeTowerModel
[[autodoc]] BridgeTowerModel
    - forward
BridgeTowerForContrastiveLearning
[[autodoc]] BridgeTowerForContrastiveLearning
    - forward
BridgeTowerForMaskedLM
[[autodoc]] BridgeTowerForMaskedLM
    - forward
BridgeTowerForImageAndTextRetrieval
[[autodoc]] BridgeTowerForImageAndTextRetrieval
    - forward.