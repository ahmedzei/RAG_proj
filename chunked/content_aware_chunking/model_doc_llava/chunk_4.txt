Simply make sure to call processor.tokenizer.padding_side = "left" before generating.

Note the model has not been explicitly trained to process multiple images in the same prompt, although this is technically possible, you may experience inaccurate results.

For better results, we recommend users to prompt the model with the correct prompt format: 

"USER: <image>\n<prompt>ASSISTANT:"
For multiple turns conversation:

"USER: <image>\n<prompt1>ASSISTANT: <answer1>USER: <prompt2>ASSISTANT: <answer2>USER: <prompt3>ASSISTANT:"
Using Flash Attention 2
Flash Attention 2 is an even faster, optimized version of the previous optimization, please refer to the Flash Attention 2 section of performance docs.
Resources
A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with BEiT.

A Google Colab demo on how to run Llava on a free-tier Google colab instance leveraging 4-bit inference.
A similar notebook showcasing batched inference.