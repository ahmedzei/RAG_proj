DiT
Overview
DiT was proposed in DiT: Self-supervised Pre-training for Document Image Transformer by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.
DiT applies the self-supervised objective of BEiT (BERT pre-training of Image Transformers) to 42 million document images, allowing for state-of-the-art results on tasks including:

document image classification: the RVL-CDIP dataset (a collection of
  400,000 images belonging to one of 16 classes).
document layout analysis: the PubLayNet dataset (a collection of more
  than 360,000 document images constructed by automatically parsing PubMed XML files).
table detection: the ICDAR 2019 cTDaR dataset (a collection of
  600 training images and 240 testing images).

The abstract from the paper is the following:
*Image Transformer has recently achieved significant progress for natural image understanding, either using supervised (ViT, DeiT, etc.) or self-supervised (BEiT, MAE, etc.) pre-training techniques.