You can also change the batch size and shuffle the dataset here if you'd like:

dataset = dataset.map(tokenize_dataset)  # doctest: +SKIP
tf_dataset = model.prepare_tf_dataset(
        dataset["train"], batch_size=16, shuffle=True, tokenizer=tokenizer
    )  # doctest: +SKIP
   

When you're ready, you can call compile and fit to start training.