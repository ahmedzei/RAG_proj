We show that these two techniques significantly improve the efficiency
of model pretraining and performance of downstream tasks.