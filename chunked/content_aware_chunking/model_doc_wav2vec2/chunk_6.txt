ðŸŒŽ
[Wav2Vec2ForCTC] is supported by a notebook on how to finetune a speech recognition model in English, and how to finetune a speech recognition model in any language.
Automatic speech recognition task guide

ðŸš€ Deploy

A blog post on how to deploy Wav2Vec2 for Automatic Speech Recognition with Hugging Face's Transformers & Amazon SageMaker.

Wav2Vec2Config
[[autodoc]] Wav2Vec2Config
Wav2Vec2CTCTokenizer
[[autodoc]] Wav2Vec2CTCTokenizer
    - call
    - save_vocabulary
    - decode
    - batch_decode
    - set_target_lang
Wav2Vec2FeatureExtractor
[[autodoc]] Wav2Vec2FeatureExtractor
    - call
Wav2Vec2Processor
[[autodoc]] Wav2Vec2Processor
    - call
    - pad
    - from_pretrained
    - save_pretrained
    - batch_decode
    - decode
Wav2Vec2ProcessorWithLM
[[autodoc]] Wav2Vec2ProcessorWithLM
    - call
    - pad
    - from_pretrained
    - save_pretrained
    - batch_decode
    - decode
Decoding multiple audios
If you are planning to decode multiple batches of audios, you should consider using [~Wav2Vec2ProcessorWithLM.batch_decode] and passing an instantiated multiprocessing.Pool.
Otherwise, [~Wav2Vec2ProcessorWithLM.batch_decode] performance will be slower than calling [~Wav2Vec2ProcessorWithLM.decode] for each audio individually, as it internally instantiates a new Pool for every call.