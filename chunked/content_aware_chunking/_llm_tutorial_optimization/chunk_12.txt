Let's give it a try.
We first load the model and tokenizer and then pass both to Transformers' pipeline object.
thon
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
model = AutoModelForCausalLM.from_pretrained("bigcode/octocoder", torch_dtype=torch.bfloat16, device_map="auto", pad_token_id=0)
tokenizer = AutoTokenizer.from_pretrained("bigcode/octocoder")
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

thon
prompt = "Question: Please write a function in Python that transforms bytes to Giga bytes.\n\nAnswer:"
result = pipe(prompt, max_new_tokens=60)[0]["generated_text"][len(prompt):]
result

Output:
Here is a Python function that transforms bytes to Giga bytes:\n\npython\ndef bytes_to_giga_bytes(bytes):\n    return bytes / 1024 / 1024 / 1024\n\n\nThis function takes a single
Nice, we can now directly use the result to convert bytes into Gigabytes.
python
def bytes_to_giga_bytes(bytes):
  return bytes / 1024 / 1024 / 1024
Let's call torch.cuda.max_memory_allocated to measure the peak GPU memory allocation.
python
bytes_to_giga_bytes(torch.cuda.max_memory_allocated())
Output:

29.0260648727417
Close enough to our back-of-the-envelope computation! We can see the number is not exactly correct as going from bytes to kilobytes requires a multiplication of 1024 instead of 1000.