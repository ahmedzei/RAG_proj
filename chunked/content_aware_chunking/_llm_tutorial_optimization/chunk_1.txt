This consequently amplifies the memory demands for inference.
In many real-world tasks, LLMs need to be given extensive contextual information.