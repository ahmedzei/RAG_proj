Note that it is not yet available through the pipeline interface.
thon
Generation as usual
prompt = system_prompt + "Question: Please write a function in Python that transforms bytes to Giga bytes.\n\nAnswer: Here"
model_inputs = tokenizer(prompt, return_tensors='pt')
generation_output = model.generate(**model_inputs, max_new_tokens=60, return_dict_in_generate=True)
decoded_output = tokenizer.batch_decode(generation_output.sequences)[0]
Piping the returned past_key_values to speed up the next conversation round
prompt = decoded_output + "\nQuestion: How can I modify the function above to return Mega bytes instead?\n\nAnswer: Here"
model_inputs = tokenizer(prompt, return_tensors='pt')
generation_output = model.generate(
  **model_inputs,
  past_key_values=generation_output.past_key_values,
  max_new_tokens=60,
  return_dict_in_generate=True
)
tokenizer.batch_decode(generation_output.sequences)[0][len(prompt):]

Output:

 is a modified version of the function that returns Mega bytes instead.
def bytes_to_megabytes(bytes):
   return bytes / 1024 / 1024
Answer: The function takes a number of bytes as input and returns the number of

Great, no additional time is spent recomputing the same key and values for the attention layer! There is however one catch.