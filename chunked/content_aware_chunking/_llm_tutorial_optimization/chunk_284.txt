Conclusion
The research community is constantly coming up with new, nifty ways to speed up inference time for ever-larger LLMs.