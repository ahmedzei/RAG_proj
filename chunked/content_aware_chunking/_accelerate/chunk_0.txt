Distributed training with ğŸ¤— Accelerate
As models get bigger, parallelism has emerged as a strategy for training larger models on limited hardware and accelerating training speed by several orders of magnitude. At Hugging Face, we created the ğŸ¤— Accelerate library to help users easily train a ğŸ¤— Transformers model on any type of distributed setup, whether it is multiple GPU's on one machine or multiple GPU's across several machines.