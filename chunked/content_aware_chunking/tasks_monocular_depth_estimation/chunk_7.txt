Here we'll use the same checkpoint as before:

from transformers import AutoImageProcessor, AutoModelForDepthEstimation
checkpoint = "vinvino02/glpn-nyu"
image_processor = AutoImageProcessor.from_pretrained(checkpoint)
model = AutoModelForDepthEstimation.from_pretrained(checkpoint)

Prepare the image input for the model using the image_processor that will take care of the necessary image transformations
such as resizing and normalization:

pixel_values = image_processor(image, return_tensors="pt").pixel_values

Pass the prepared inputs through the model:

import torch
with torch.no_grad():
     outputs = model(pixel_values)
     predicted_depth = outputs.predicted_depth

Visualize the results:

import numpy as np
interpolate to original size
prediction = torch.nn.functional.interpolate(
     predicted_depth.unsqueeze(1),
     size=image.size[::-1],
     mode="bicubic",
     align_corners=False,
 ).squeeze()
output = prediction.numpy()
formatted = (output * 255 / np.max(output)).astype("uint8")
depth = Image.fromarray(formatted)
depth