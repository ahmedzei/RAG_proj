The original code can be found here.
Usage tips

T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which
each task is converted into a text-to-text format. T5 works well on a variety of tasks out-of-the-box by prepending a
different prefix to the input corresponding to each task, e.g., for translation: translate English to German: ,
for summarization: summarize: .
The pretraining includes both supervised and self-supervised training.