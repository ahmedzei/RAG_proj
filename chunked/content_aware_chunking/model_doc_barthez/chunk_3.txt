We also continue the pretraining of an already
pretrained multilingual BART on BARThez's corpus, and we show that the resulting model, which we call mBARTHez,
provides a significant boost over vanilla BARThez, and is on par with or outperforms CamemBERT and FlauBERT.
This model was contributed by moussakam. The Authors' code can be found here.
 
BARThez implementation is the same as BART, except for tokenization. Refer to BART documentation for information on 
configuration classes and their parameters.