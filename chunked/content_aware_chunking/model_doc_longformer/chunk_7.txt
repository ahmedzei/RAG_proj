Also note
that every "locally" attending token not only attends to tokens within its window \(w\), but also to all "globally"
attending tokens so that global attention is symmetric.
The user can define which tokens attend "locally" and which tokens attend "globally" by setting the tensor
global_attention_mask at run-time appropriately.