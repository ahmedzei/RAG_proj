Most tokens only
attend "locally" to each other meaning that each token attends to its \(\frac{1}{2} w\) previous tokens and
\(\frac{1}{2} w\) succeeding tokens with \(w\) being the window length as defined in
config.attention_window. Note that config.attention_window can be of type List to define a
different \(w\) for each layer.