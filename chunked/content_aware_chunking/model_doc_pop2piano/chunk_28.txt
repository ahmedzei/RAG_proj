audio1, sr1 = librosa.load("", sr=44100)
audio2, sr2 = librosa.load("", sr=44100)
model = Pop2PianoForConditionalGeneration.from_pretrained("sweetcocoa/pop2piano")
feature_extractor = Pop2PianoFeatureExtractor.from_pretrained("sweetcocoa/pop2piano")
tokenizer = Pop2PianoTokenizer.from_pretrained("sweetcocoa/pop2piano")
inputs = feature_extractor(
     audio=[audio1, audio2], 
     sampling_rate=[sr1, sr2], 
     return_attention_mask=True, 
     return_tensors="pt",
 )
Since we now generating in batch(2 audios) we must pass the attention_mask
model_output = model.generate(
     input_features=inputs["input_features"],
     attention_mask=inputs["attention_mask"],
     composer="composer1",
 )
tokenizer_output = tokenizer.batch_decode(
     token_ids=model_output, feature_extractor_output=inputs
 )["pretty_midi_objects"]
Since we now have 2 generated MIDI files
tokenizer_output[0].write("./Outputs/midi_output1.mid")
tokenizer_output[1].write("./Outputs/midi_output2.mid")

Pop2PianoConfig
[[autodoc]] Pop2PianoConfig
Pop2PianoFeatureExtractor
[[autodoc]] Pop2PianoFeatureExtractor
    - call
Pop2PianoForConditionalGeneration
[[autodoc]] Pop2PianoForConditionalGeneration
    - forward
    - generate
Pop2PianoTokenizer
[[autodoc]] Pop2PianoTokenizer
    - call
Pop2PianoProcessor
[[autodoc]] Pop2PianoProcessor
    - call