`torch.float16``)
To load and run a model using Flash Attention 2, refer to the snippet below:
thon

import torch
from transformers import OPTForCausalLM, GPT2Tokenizer
device = "cuda" # the device to load the model onto
model = OPTForCausalLM.from_pretrained("facebook/opt-350m", torch_dtype=torch.float16, attn_implementation="flash_attention_2")
tokenizer = GPT2Tokenizer.from_pretrained("facebook/opt-350m")
prompt = ("A chat between a curious human and the Statue of Liberty.\n\nHuman: What is your name?\nStatue: I am the "
              "Statue of Liberty.\nHuman: Where do you live?\nStatue: New York City.\nHuman: How long have you lived "
              "there?")
model_inputs = tokenizer([prompt], return_tensors="pt").to(device)
model.to(device)
generated_ids = model.generate(**model_inputs, max_new_tokens=30, do_sample=False)
tokenizer.batch_decode(generated_ids)[0]
'A chat between a curious human and the Statue of Liberty.\n\nHuman: What is your name?\nStatue: I am the Statue of Liberty.\nHuman: Where do you live?\nStatue: New York City.\nHuman: How long have you lived there?\nStatue: I have lived here for about a year.\nHuman: What is your favorite place to eat?\nStatue: I love'

Expected speedups
Below is an expected speedup diagram that compares pure inference time between the native implementation in transformers using facebook/opt-2.7b checkpoint and the Flash Attention 2 version of the model using two different sequence lengths.

Below is an expected speedup diagram that compares pure inference time between the native implementation in transformers using facebook/opt-350m checkpoint and the Flash Attention 2 version of the model using two different sequence lengths.

OPTConfig
[[autodoc]] OPTConfig

OPTModel
[[autodoc]] OPTModel
    - forward
OPTForCausalLM
[[autodoc]] OPTForCausalLM
    - forward
OPTForSequenceClassification
[[autodoc]] OPTForSequenceClassification
    - forward
OPTForQuestionAnswering
[[autodoc]] OPTForQuestionAnswering
    - forward

TFOPTModel
[[autodoc]] TFOPTModel
    - call
TFOPTForCausalLM
[[autodoc]] TFOPTForCausalLM
    - call

FlaxOPTModel
[[autodoc]] FlaxOPTModel
    - call
FlaxOPTForCausalLM
[[autodoc]] FlaxOPTForCausalLM
    - call

.