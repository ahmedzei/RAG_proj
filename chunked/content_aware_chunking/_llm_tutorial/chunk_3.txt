Autoregressive generation with LLMs is also resource-intensive and should be executed on a GPU for adequate throughput.

First, you need to load the model.

from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(
     "mistralai/Mistral-7B-v0.1", device_map="auto", load_in_4bit=True
 )

You'll notice two flags in the from_pretrained call:

device_map ensures the model is moved to your GPU(s)
load_in_4bit applies 4-bit dynamic quantization to massively reduce the resource requirements

There are other ways to initialize a model, but this is a good baseline to begin with an LLM.
Next, you need to preprocess your text input with a tokenizer.

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
model_inputs = tokenizer(["A list of colors: red, blue"], return_tensors="pt").to("cuda")

The model_inputs variable holds the tokenized text input, as well as the attention mask.