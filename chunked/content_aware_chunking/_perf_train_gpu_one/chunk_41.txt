However, if the model doesn't fit onto a single GPU or you can't fit a small batch, you can 
leverage DeepSpeed ZeRO + CPU Offload, or NVMe Offload for much larger models. In this case, you need to separately
install the library, then follow one of the guides to create a configuration file 
and launch DeepSpeed: 

For an in-depth guide on DeepSpeed integration with [Trainer], review the corresponding documentation, specifically the 
section for a single GPU.