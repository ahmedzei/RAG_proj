As part of 
hyperparameter tuning, you should determine which batch size yields the best results and then optimize resources accordingly.
The methods and tools covered in this guide can be classified based on the effect they have on the training process:
| Method/tool                                                | Improves training speed | Optimizes memory utilization |
|:-----------------------------------------------------------|:------------------------|:-----------------------------|
| Batch size choice                    | Yes                     | Yes                          |
| Gradient accumulation            | No                      | Yes                          |
| Gradient checkpointing          | No                      | Yes                          |
| Mixed precision training      | Yes                     | (No)                         |
| Optimizer choice                      | Yes                     | Yes                          |
| Data preloading                        | Yes                     | No                           |
| DeepSpeed Zero                          | No                      | Yes                          |
| torch.compile                       | Yes                     | No                           |
| Parameter-Efficient Fine Tuning (PEFT)            | No                      | Yes                          |

Note: when using mixed precision with a small model and a large batch size, there will be some memory savings but with a 
large model and a small batch size, the memory use will be larger.

You can combine the above methods to get a cumulative effect.