If you're already using fp16 or bf16 mixed precision it may help with the throughput as well.
You can enable this mode in the ðŸ¤— Trainer:
python
TrainingArguments(tf32=True, **default_args)

tf32 can't be accessed directly via tensor.to(dtype=torch.tf32) because it is an internal CUDA data type.