Alternatively, use ðŸ¤— Accelerate to gain full control over the training loop. Find the ðŸ¤— Accelerate example 
further down in this guide.
While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can 
result in a more pronounced training slowdown. Consider the following example. Let's say, the per_device_train_batch_size=4 
without gradient accumulation hits the GPU's limit.