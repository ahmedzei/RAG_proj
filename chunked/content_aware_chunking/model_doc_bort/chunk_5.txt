The original code can be found here.
Usage tips

BORT's model architecture is based on BERT, refer to BERT's documentation page for the
  model's API reference as well as usage examples.
BORT uses the RoBERTa tokenizer instead of the BERT tokenizer, refer to RoBERTa's documentation page for the tokenizer's API reference as well as usage examples.
BORT requires a specific fine-tuning algorithm, called Agora ,
  that is sadly not open-sourced yet.