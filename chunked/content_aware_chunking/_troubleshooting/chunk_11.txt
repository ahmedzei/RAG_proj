Now the output of the second sequence matches its actual output:

By default, the tokenizer creates an attention_mask for you based on your specific tokenizer's defaults.

attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0]])
output = model(input_ids, attention_mask=attention_mask)
print(output.logits)
tensor([[ 0.0082, -0.2307],
        [-0.1008, -0.4061]], grad_fn=)

ðŸ¤— Transformers doesn't automatically create an attention_mask to mask a padding token if it is provided because:

Some models don't have a padding token.
For some use-cases, users want a model to attend to a padding token.

ValueError: Unrecognized configuration class XYZ for this kind of AutoModel
Generally, we recommend using the [AutoModel] class to load pretrained instances of models.