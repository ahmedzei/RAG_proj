A series of models called 'SwiftFormer' is built based on this, which achieves state-of-the-art performance in terms of both accuracy and mobile inference speed. Even their small variant achieves 78.5% top-1 ImageNet1K accuracy with only 0.8 ms latency on iPhone 14, which is more accurate and 2Ã— faster compared to MobileViT-v2.
The abstract from the paper is the following:
Self-attention has become a defacto choice for capturing global context in various vision applications.