Dropout should be re-enabled during fine-tuning.

Pre-trained on C4 only without mixing in the downstream tasks.

No parameter sharing between the embedding and classifier layer.

"xl" and "xxl" replace "3B" and "11B". The model shapes are a bit different - larger d_model and smaller
  num_heads and d_ff.

Note: T5 Version 1.1 was only pre-trained on C4 excluding any supervised
training. Therefore, this model has to be fine-tuned before it is usable on a downstream task, unlike the original T5
model.