For example, to distribute 1GB of memory to the first GPU and 2GB of memory to the second GPU:
py
max_memory_mapping = {0: "1GB", 1: "2GB"}
model_name = "bigscience/bloom-3b"
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_8bit=True, max_memory=max_memory_mapping
)

Feel free to try running a 11 billion parameter T5 model or the 3 billion parameter BLOOM model for inference on Google Colab's free tier GPUs!

ðŸ¤— Optimum

Learn more details about using ORT with ðŸ¤— Optimum in the Accelerated inference on NVIDIA GPUs and Accelerated inference on AMD GPUs guides.