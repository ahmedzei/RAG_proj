Extractive question answering is typically evaluated using F1/exact match.
If you'd like to implement it yourself, check out the Question Answering chapter
of the Hugging Face course for inspiration.
Train
Congratulations! You've successfully navigated the toughest part of this guide and now you are ready to train your own model.
Training involves the following steps:
* Load the model with [AutoModelForDocumentQuestionAnswering] using the same checkpoint as in the preprocessing.
* Define your training hyperparameters in [TrainingArguments].
* Define a function to batch examples together, here the [DefaultDataCollator] will do just fine
* Pass the training arguments to [Trainer] along with the model, dataset, and data collator.
* Call [~Trainer.train] to finetune your model.

from transformers import AutoModelForDocumentQuestionAnswering
model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_checkpoint)

In the [TrainingArguments] use output_dir to specify where to save your model, and configure hyperparameters as you see fit.
If you wish to share your model with the community, set push_to_hub to True (you must be signed in to Hugging Face to upload your model).
In this case the output_dir will also be the name of the repo where your model checkpoint will be pushed.

from transformers import TrainingArguments
REPLACE THIS WITH YOUR REPO ID
repo_id = "MariaK/layoutlmv2-base-uncased_finetuned_docvqa"
training_args = TrainingArguments(
     output_dir=repo_id,
     per_device_train_batch_size=4,
     num_train_epochs=20,
     save_steps=200,
     logging_steps=50,
     evaluation_strategy="steps",
     learning_rate=5e-5,
     save_total_limit=2,
     remove_unused_columns=False,
     push_to_hub=True,
 )

Define a simple data collator to batch examples together.

from transformers import DefaultDataCollator
data_collator = DefaultDataCollator()

Finally, bring everything together, and call [~Trainer.train]:

from transformers import Trainer
trainer = Trainer(
     model=model,
     args=training_args,
     data_collator=data_collator,
     train_dataset=encoded_train_dataset,
     eval_dataset=encoded_test_dataset,
     tokenizer=processor,
 )
trainer.train()

To add the final model to ðŸ¤— Hub, create a model card and call push_to_hub:

trainer.create_model_card()
trainer.push_to_hub()

Inference
Now that you have finetuned a LayoutLMv2 model, and uploaded it to the ðŸ¤— Hub, you can use it for inference.