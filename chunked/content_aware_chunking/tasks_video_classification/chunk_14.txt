Starting with the training set: 
 

train_transform = Compose(
     [
         ApplyTransformToKey(
             key="video",
             transform=Compose(
                 [
                     UniformTemporalSubsample(num_frames_to_sample),
                     Lambda(lambda x: x / 255.0),
                     Normalize(mean, std),
                     RandomShortSideScale(min_size=256, max_size=320),
                     RandomCrop(resize_to),
                     RandomHorizontalFlip(p=0.5),
                 ]
             ),
         ),
     ]
 )
train_dataset = pytorchvideo.data.Ucf101(
     data_path=os.path.join(dataset_root_path, "train"),
     clip_sampler=pytorchvideo.data.make_clip_sampler("random", clip_duration),
     decode_audio=False,
     transform=train_transform,
 )

The same sequence of workflow can be applied to the validation and evaluation sets: 
 

val_transform = Compose(
     [
         ApplyTransformToKey(
             key="video",
             transform=Compose(
                 [
                     UniformTemporalSubsample(num_frames_to_sample),
                     Lambda(lambda x: x / 255.0),
                     Normalize(mean, std),
                     Resize(resize_to),
                 ]
             ),
         ),
     ]
 )
val_dataset = pytorchvideo.data.Ucf101(
     data_path=os.path.join(dataset_root_path, "val"),
     clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
     decode_audio=False,
     transform=val_transform,
 )
test_dataset = pytorchvideo.data.Ucf101(
     data_path=os.path.join(dataset_root_path, "test"),
     clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
     decode_audio=False,
     transform=val_transform,
 )

Note: The above dataset pipelines are taken from the official PyTorchVideo example.