**STEP 4: Train (fine-tune) the model

You can then fine-tune [TapasForQuestionAnswering] as follows (shown here for the weak supervision for aggregation case):

from transformers import TapasConfig, TapasForQuestionAnswering, AdamW
this is the default WTQ configuration
config = TapasConfig(
     num_aggregation_labels=4,
     use_answer_as_supervision=True,
     answer_loss_cutoff=0.664694,
     cell_selection_preference=0.207951,
     huber_loss_delta=0.121194,
     init_cell_selection_weights_to_zero=True,
     select_one_column=True,
     allow_empty_column_selection=False,
     temperature=0.0352513,
 )
model = TapasForQuestionAnswering.from_pretrained("google/tapas-base", config=config)
optimizer = AdamW(model.parameters(), lr=5e-5)
model.train()
for epoch in range(2):  # loop over the dataset multiple times
     for batch in train_dataloader:
         # get the inputs;
         input_ids = batch["input_ids"]
         attention_mask = batch["attention_mask"]
         token_type_ids = batch["token_type_ids"]
         labels = batch["labels"]
         numeric_values = batch["numeric_values"]
         numeric_values_scale = batch["numeric_values_scale"]
         float_answer = batch["float_answer"]

         # zero the parameter gradients
         optimizer.zero_grad()
         # forward + backward + optimize
         outputs = model(
             input_ids=input_ids,
             attention_mask=attention_mask,
             token_type_ids=token_type_ids,
             labels=labels,
             numeric_values=numeric_values,
             numeric_values_scale=numeric_values_scale,
             float_answer=float_answer,
         )
         loss = outputs.loss
         loss.backward()
         optimizer.step()
``
</pt>
<tf>
You can then fine-tune [TFTapasForQuestionAnswering`] as follows (shown here for the weak supervision for aggregation case):

import tensorflow as tf
from transformers import TapasConfig, TFTapasForQuestionAnswering
this is the default WTQ configuration
config = TapasConfig(
     num_aggregation_labels=4,
     use_answer_as_supervision=True,
     answer_loss_cutoff=0.664694,
     cell_selection_preference=0.207951,
     huber_loss_delta=0.121194,
     init_cell_selection_weights_to_zero=True,
     select_one_column=True,
     allow_empty_column_selection=False,
     temperature=0.0352513,
 )
model = TFTapasForQuestionAnswering.from_pretrained("google/tapas-base", config=config)
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
for epoch in range(2):  # loop over the dataset multiple times
     for batch in train_dataloader:
         # get the inputs;
         input_ids = batch[0]
         attention_mask = batch[1]
         token_type_ids = batch[4]
         labels = batch[-1]
         numeric_values = batch[2]
         numeric_values_scale = batch[3]
         float_answer = batch[6]

         # forward + backward + optimize
         with tf.GradientTape() as tape:
             outputs = model(
                 input_ids=input_ids,
                 attention_mask=attention_mask,
                 token_type_ids=token_type_ids,
                 labels=labels,
                 numeric_values=numeric_values,
                 numeric_values_scale=numeric_values_scale,
                 float_answer=float_answer,
             )
         grads = tape.gradient(outputs.loss, model.trainable_weights)
         optimizer.apply_gradients(zip(grads, model.trainable_weights))

Usage: inference

Here we explain how you can use [TapasForQuestionAnswering] or [TFTapasForQuestionAnswering] for inference (i.e.