Here's an example:

from transformers import TapasTokenizer
import pandas as pd
model_name = "google/tapas-base"
tokenizer = TapasTokenizer.from_pretrained(model_name)
data = {"Actors": ["Brad Pitt", "Leonardo Di Caprio", "George Clooney"], "Number of movies": ["87", "53", "69"]}
queries = [
     "What is the name of the first actor?",
     "How many movies has George Clooney played in?",
     "What is the total number of movies?",
 ]
answer_coordinates = [[(0, 0)], [(2, 1)], [(0, 1), (1, 1), (2, 1)]]
answer_text = [["Brad Pitt"], ["69"], ["209"]]
table = pd.DataFrame.from_dict(data)
inputs = tokenizer(
     table=table,
     queries=queries,
     answer_coordinates=answer_coordinates,
     answer_text=answer_text,
     padding="max_length",
     return_tensors="pt",
 )
inputs
{'input_ids': tensor([[  ]]), 'attention_mask': tensor([[]]), 'token_type_ids': tensor([[[]]]),
'numeric_values': tensor([[  ]]), 'numeric_values_scale: tensor([[  ]]), labels: tensor([[  ]])}

Note that [TapasTokenizer] expects the data of the table to be text-only.