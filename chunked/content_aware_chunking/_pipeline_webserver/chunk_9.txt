This wasn't done here because the code is a lot more
complex (mostly because threads and async and queues don't play nice together).
But ultimately it does the same thing.
This would be important if the inference of single items were long (> 1s) because 
in this case, it means every query during inference would have to wait for 1s before
even receiving an error.
Dynamic batching
In general, batching is not necessarily an improvement over passing 1 item at 
a time (see batching details for more information).