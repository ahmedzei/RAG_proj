The launcher waits until both nodes are synchronized before launching the training.

python -m torch.run --nproc_per_node=8 --nnode=2 --node_rank=0 --master_addr=hostname1 \
--master_port=9901 your_program.py <normal cl args> --deepspeed ds_config.json

For the deepspeed launcher, start by creating a hostfile.

hostname1 slots=8
hostname2 slots=8
Then you can launch the training with the following command.