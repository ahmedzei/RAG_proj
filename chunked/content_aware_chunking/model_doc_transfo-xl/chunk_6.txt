The
  original implementation trains on SQuAD with padding on the left, therefore the padding defaults are set to left.
Transformer-XL is one of the few models that has no sequence length limit.
Same as a regular GPT model, but introduces a recurrence mechanism for two consecutive segments (similar to a regular RNNs with two consecutive inputs).