By stacking multiple attention layers, the receptive field can be increased to multiple previous segments.
This changes the positional embeddings to positional relative embeddings (as the regular positional embeddings would give the same results in the current input and the current hidden state at a given position) and needs to make some adjustments in the way attention scores are computed.

TransformerXL does not work with torch.nn.DataParallel due to a bug in PyTorch, see issue #36035

Resources

Text classification task guide
Causal language modeling task guide

TransfoXLConfig
[[autodoc]] TransfoXLConfig
TransfoXLTokenizer
[[autodoc]] TransfoXLTokenizer
    - save_vocabulary
TransfoXL specific outputs
[[autodoc]] models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput
[[autodoc]] models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput
[[autodoc]] models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput
[[autodoc]] models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput

TransfoXLModel
[[autodoc]] TransfoXLModel
    - forward
TransfoXLLMHeadModel
[[autodoc]] TransfoXLLMHeadModel
    - forward
TransfoXLForSequenceClassification
[[autodoc]] TransfoXLForSequenceClassification
    - forward

TFTransfoXLModel
[[autodoc]] TFTransfoXLModel
    - call
TFTransfoXLLMHeadModel
[[autodoc]] TFTransfoXLLMHeadModel
    - call
TFTransfoXLForSequenceClassification
[[autodoc]] TFTransfoXLForSequenceClassification
    - call

Internal Layers
[[autodoc]] AdaptiveEmbedding
[[autodoc]] TFAdaptiveEmbedding.