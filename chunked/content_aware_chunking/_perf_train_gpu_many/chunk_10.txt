DDP performs only a single communication per batch - sending gradients, while DP performs five different data exchanges per batch.
DDP copies data using torch.distributed, while DP copies data within 
the process via Python threads (which introduces limitations associated with GIL). As a result, DistributedDataParallel (DDP) is generally faster than DataParallel (DP) unless you have slow GPU card inter-connectivity.
2.