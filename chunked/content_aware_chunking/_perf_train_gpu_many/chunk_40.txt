If you need a TP degree of 8, you need to use
nodes that have at least 8 GPUs.
This section is based on the original much more detailed TP overview.
by @anton-l.
Alternative names:
- DeepSpeed calls it tensor slicing
Implementations:
- Megatron-LM has an internal implementation, as it's very model-specific
- parallelformers (only inference at the moment)
- SageMaker - this is a proprietary solution that can only be used on AWS.
- OSLO has the tensor parallelism implementation based on the Transformers.
SageMaker combines TP with DP for a more efficient processing.
ðŸ¤— Transformers status:
- core: not yet implemented in the core
- but if you want inference parallelformers provides this support for most of our models.