BERT is pretrained with two objectives: masked language modeling and next-sentence prediction.