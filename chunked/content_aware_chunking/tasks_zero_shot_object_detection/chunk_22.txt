Since the image processor resized images before
feeding them to the model, you need to use the [~OwlViTImageProcessor.post_process_object_detection] method to make sure the predicted bounding
boxes have the correct coordinates relative to the original image:

import torch
with torch.no_grad():
     outputs = model(**inputs)
     target_sizes = torch.tensor([im.size[::-1]])
     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]
draw = ImageDraw.Draw(im)
scores = results["scores"].tolist()
labels = results["labels"].tolist()
boxes = results["boxes"].tolist()
for box, score, label in zip(boxes, scores, labels):
     xmin, ymin, xmax, ymax = box
     draw.rectangle((xmin, ymin, xmax, ymax), outline="red", width=1)
     draw.text((xmin, ymin), f"{text_queries[label]}: {round(score,2)}", fill="white")
im

Batch processing
You can pass multiple sets of images and text queries to search for different (or same) objects in several images.