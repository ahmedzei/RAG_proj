Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example.