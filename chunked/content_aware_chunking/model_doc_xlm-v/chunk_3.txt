XLM-V outperforms XLM-R on every task we
tested on ranging from natural language inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), and
named entity recognition (WikiAnn) to low-resource tasks (Americas NLI, MasakhaNER).
This model was contributed by stefan-it, including detailed experiments with XLM-V on downstream tasks.
The experiments repository can be found here.
Usage tips

XLM-V is compatible with the XLM-RoBERTa model architecture, only model weights from fairseq
  library had to be converted.
The XLMTokenizer implementation is used to load the vocab and performs tokenization.

A XLM-V (base size) model is available under the facebook/xlm-v-base identifier.

XLM-V architecture is the same as XLM-RoBERTa, refer to XLM-RoBERTa documentation for API reference, and examples.
.