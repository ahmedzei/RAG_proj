These were also tested against the bitsandbytes quantization methods as well as a native fp16 model.

forward peak memory/batch size

generate peak memory/batch size

generate throughput/batch size

forward latency/batch size

The benchmarks indicate AWQ quantization is the fastest for inference, text generation, and has the lowest peak memory for text generation. However, AWQ has the largest forward latency per batch size.