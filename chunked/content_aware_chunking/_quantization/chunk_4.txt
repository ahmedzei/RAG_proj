Make sure to install it to run the models (note aqlm works only with python>=3.10):

pip install aqlm[gpu,cpu]
The library provides efficient kernels for both GPU and CPU inference.
The instructions on how to quantize models yourself, as well as all the relevant code can be found in the corresponding GitHub repository.
AQLM configurations
AQLM quantization setpus vary mainly on the number of codebooks used as well as codebook sizes in bits.