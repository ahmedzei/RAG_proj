thon
from transformers import BarkModel
import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
load in fp16 and use Flash Attention 2
model = BarkModel.from_pretrained("suno/bark-small", torch_dtype=torch.float16, attn_implementation="flash_attention_2").to(device)
enable CPU offload
model.enable_cpu_offload()

Find out more on inference optimization techniques here.