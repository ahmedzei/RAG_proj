Saving all activations from the forward pass in order to compute the gradients during the backward pass can result in 
significant memory overhead.