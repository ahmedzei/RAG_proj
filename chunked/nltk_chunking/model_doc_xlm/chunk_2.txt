In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining.