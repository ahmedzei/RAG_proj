This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low-resource languages.