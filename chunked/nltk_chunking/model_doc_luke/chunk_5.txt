Our model is trained using a new pretraining task based on the masked language model of BERT.