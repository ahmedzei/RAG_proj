As an alternative, we propose a
more sample-efficient pretraining task called replaced token detection.