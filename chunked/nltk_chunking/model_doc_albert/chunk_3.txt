The abstract from the paper is the following:
Increasing model size when pretraining natural language representations often results in improved performance on
downstream tasks.