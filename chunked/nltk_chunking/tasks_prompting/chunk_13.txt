You can customize how your LLM selects each of the subsequent tokens when generating 
the text without modifying any of the trainable parameters.