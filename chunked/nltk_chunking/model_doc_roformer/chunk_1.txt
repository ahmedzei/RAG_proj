The abstract from the paper is the following:
Position encoding in transformer architecture provides supervision for dependency modeling between elements at
different positions in the sequence.