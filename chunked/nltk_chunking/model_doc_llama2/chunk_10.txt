Usage tips

The Llama2 models were trained using bfloat16, but the original inference uses float16.