Thus, in the context of knowledge distillation, KL divergence is useful.