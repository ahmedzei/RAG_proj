We explore a
general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) â€” models which combine pre-trained
parametric and non-parametric memory for language generation.