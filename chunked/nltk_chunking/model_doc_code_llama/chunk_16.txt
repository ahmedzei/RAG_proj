bfloat16: Code Llama was trained with this precision, so we recommend using it for further training or fine-tuning.