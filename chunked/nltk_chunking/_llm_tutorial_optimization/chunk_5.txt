This necessitates the model's capability to manage very long input sequences during inference.