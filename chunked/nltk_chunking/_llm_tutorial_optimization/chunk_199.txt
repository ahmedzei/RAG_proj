Both RoPE and ALiBi are relative positional embeddings that are not learned during training, but instead are based on the following intuitions:
 -   Positional cues about the text inputs should be given directly to the \( QK^T \) matrix of the self-attention layer
 -   The LLM should be incentivized to learn a constant relative distance positional encodings have to each other
 -   The further text input tokens are from each other, the lower the probability of their query-value probability.