To give some examples of how much VRAM it roughly takes to load a model in bfloat16:

GPT3 requires 2 * 175 GB = 350 GB VRAM
Bloom requires 2 * 176 GB = 352 GB VRAM
Llama-2-70b requires 2 * 70 GB = 140 GB VRAM
Falcon-40b requires 2 * 40 GB = 80 GB VRAM
MPT-30b requires 2 * 30 GB = 60 GB VRAM
bigcode/starcoder requires 2 * 15.5 = 31 GB VRAM

As of writing this document, the largest GPU chip on the market is the A100 & H100 offering 80GB of VRAM.