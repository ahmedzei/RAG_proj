The positional embeddings
The key-value cache

Let's go over each component in more detail
3.1 Improving positional embeddings of LLMs
Self-attention puts each token in relation to each other's tokens.