Model can be quantized to even 3 or 2 bits with an acceptable loss in performance as shown in the recent GPTQ paper ðŸ¤¯.