If you are latency constrained (live product doing inference), don't batch.