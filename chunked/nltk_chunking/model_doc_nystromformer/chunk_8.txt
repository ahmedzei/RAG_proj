On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystr√∂mformer performs
favorably relative to other efficient self-attention methods.