But the input actually hasn't been tokenized yet and you'll need to set is_split_into_words=True to tokenize the words into subwords.