Model Details
Mixtral-45B is a decoder-based LM with the following architectural choices:

Mixtral is a Mixture of Expert (MOE) model with 8 experts per MLP, with a total of 45B paramateres but the compute required is the same as a 14B model.