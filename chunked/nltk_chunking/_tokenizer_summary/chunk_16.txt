Taking punctuation into account, tokenizing our exemplary text would give:
["Don", "'", "t", "you", "love", "ðŸ¤—", "Transformers", "?