We train a Transformer-based masked language model on one hundred
languages, using more than two terabytes of filtered CommonCrawl data.