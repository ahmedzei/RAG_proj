We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size.