2").to(device)

Expected speedups
Below is an expected speedup diagram that compares pure inference time between the native implementation in transformers using stockmark/gpt-neox-japanese-1.4b checkpoint and the Flash Attention 2 version of the model using a sequence length of 2048.

Resources

Causal language modeling task guide

GPTNeoXConfig
[[autodoc]] GPTNeoXConfig
GPTNeoXTokenizerFast
[[autodoc]] GPTNeoXTokenizerFast
GPTNeoXModel
[[autodoc]] GPTNeoXModel
    - forward
GPTNeoXForCausalLM
[[autodoc]] G