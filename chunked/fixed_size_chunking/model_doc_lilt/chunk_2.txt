uages with the corresponding off-the-shelf monolingual/multilingual pre-trained textual models. Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks, which enables language-independent benefit from the pre-training of document layout structure.

 LiLT architecture. Taken from the original paper. 
This model was contributed by nielsr.
The original code can be found here.
Usage tips

To combine the Langua