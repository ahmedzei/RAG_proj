erformance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We
will make XLM-R code, data, and models publicly available.
This model was contributed by stefan-it. The original code can be found here.
Usage tips

XLM-RoBERTa is a multilingual model trained on 100 different languages. Unlike some XLM multilingual models, it does
  not require lang tensors to understand which language is used, and should be able to determine the correct
  language from the input ids.
Us