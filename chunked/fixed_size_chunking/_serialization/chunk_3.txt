run with ONNX Runtime via ORTModelForXXX classes,
which follow the same AutoModel API as the one you are used to in ðŸ¤— Transformers.
- run with optimized inference pipelines,
which has the same API as the [pipeline] function in ðŸ¤— Transformers. 
ðŸ¤— Optimum provides support for the ONNX export by leveraging configuration objects. These configuration objects come 
ready-made for a number of model architectures, and are designed to be easily extendable to other architectures.
For the list of ready-made configurat