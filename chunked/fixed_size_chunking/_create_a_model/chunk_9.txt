ter results faster, while using only a fraction of the resources required for training.
Create a pretrained model with [~PreTrainedModel.from_pretrained]:

model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased")

When you load pretrained weights, the default model configuration is automatically loaded if the model is provided by ðŸ¤— Transformers. However, you can still replace - some or all of - the default model configuration attributes with your own if you'd like:

model = DistilBertMo