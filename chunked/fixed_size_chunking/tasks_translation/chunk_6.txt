th the ðŸ¤— Evaluate library. For this task, load the SacreBLEU metric (see the ðŸ¤— Evaluate quick tour to learn more about how to load and compute a metric):

import evaluate
metric = evaluate.load("sacrebleu")

Then create a function that passes your predictions and labels to [~evaluate.EvaluationModule.compute] to calculate the SacreBLEU score:

import numpy as np
def postprocess_text(preds, labels):
     preds = [pred.strip() for pred in preds]
     labels = [[label.strip()] for label in labels]

     return