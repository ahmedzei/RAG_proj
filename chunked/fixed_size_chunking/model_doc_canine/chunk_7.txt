tputs.last_hidden_state

For batched inference and training, it is however recommended to make use of the tokenizer (to pad/truncate all
sequences to the same length):
thon

from transformers import CanineTokenizer, CanineModel
model = CanineModel.from_pretrained("google/canine-c")
tokenizer = CanineTokenizer.from_pretrained("google/canine-c")
inputs = ["Life is like a box of chocolates.", "You never know what you gonna get."]
encoding = tokenizer(inputs, padding="longest", truncation=True, return_tensors="