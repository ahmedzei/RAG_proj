tive position embeddings which only considers distance between sequence elements rather than absolute positions. Please refer to Self-Attentionwith Relative Position Representations (Shaw et al.) for more details.
- the use of a causal depth-wise convolution instead of a non-causal one.
Generation process
Here's how the generation process works:

Input text or speech is processed through its specific encoder.
A decoder creates text tokens in the desired language.
If speech generation is required, the second