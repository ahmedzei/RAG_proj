el(DP) - this approach will result in fewer communications, but requires significant changes to the model

When you have slow inter-node connectivity and still low on GPU memory:

Employ a combination of DataParallel(DP) with PipelineParallel(PP), TensorParallel(TP), and ZeRO.

In the following sections of this guide we dig deeper into how these different parallelism methods work.
Data Parallelism
Even with only 2 GPUs, you can readily leverage the accelerated training capabilities offered by PyTorch's buil