the main process replicates the model once from GPU 0 to the rest of GPUs
Then for each batch:
Each GPU directly consumes its mini-batch of data.
During backward, once the local gradients are ready, they are averaged across all processes.

DP:
For each batch:
   1. GPU 0 reads the batch of data and then sends a mini-batch to each GPU.
   2. The up-to-date model is replicated from GPU 0 to each GPU. 
   3. forward is executed, and output from each GPU is sent to GPU 0 to compute the loss.
   4. The loss is d