encoded input sequence) and labels (which are the input_ids of the encoded
target sequence). The model will automatically create the decoder_input_ids based on the labels, by
shifting them one position to the right and prepending the config.decoder_start_token_id, which for T5 is
equal to 0 (i.e. the id of the pad token). Also note the task prefix: we prepend the input sequence with 'translate
English to German: ' before encoding it. This will help in improving the performance, as this task prefix was used
