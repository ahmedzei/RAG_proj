 + doc for doc in examples["text"]]
     model_inputs = tokenizer(inputs, max_length=1024, truncation=True)

     labels = tokenizer(text_target=examples["summary"], max_length=128, truncation=True)
     model_inputs["labels"] = labels["input_ids"]
     return model_inputs

To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [~datasets.Dataset.map] method. You can speed up the map function by setting batched=True to process multiple elements of the dataset at once:

tokenized_billsum