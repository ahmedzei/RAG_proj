e when the following arguments are passed: --fp16, --fp16_backend apex or --fp16_opt_level 01.
yaml
{
    "amp": {
        "enabled": "auto",
        "opt_level": "auto"
    }
}

To use bf16, you'll need at least DeepSpeed==0.6.0. bf16 has the same dynamic range as fp32 and doesnâ€™t require loss scaling. However, if you use gradient accumulation with bf16, gradients are accumulated in bf16 which may not be desired because this format's low precision can lead to lossy accumulation.
bf16 can be setup in the co