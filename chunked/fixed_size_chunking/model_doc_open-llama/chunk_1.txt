developer s-JoL.
The model is mainly based on LLaMA with some modifications, incorporating memory-efficient attention from Xformers, stable embedding from Bloom, and shared input-output embedding from PaLM.
And the model is pre-trained on both Chinese and English, which gives it better performance on Chinese language tasks.
This model was contributed by s-JoL.
The original code was released on GitHub by s-JoL, but is now removed.
OpenLlamaConfig
[[autodoc]] OpenLlamaConfig
OpenLlamaModel
[[autodoc]] OpenLla