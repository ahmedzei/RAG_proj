aining for open-world localization, similar to what has been seen for image classification and language modelling.

 OWLv2 high-level overview. Taken from the original paper. 
This model was contributed by nielsr.
The original code can be found here.
Usage example
OWLv2 is, just like its predecessor OWL-ViT, a zero-shot text-conditioned object detection model. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text feature