nsistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on
WikiHop and TriviaQA.
This model was contributed by beltagy. The Authors' code can be found here.
Usage tips

Since the Longformer is based on RoBERTa, it doesn't have token_type_ids. You don't need to indicate which
  token belongs to which segment. Just separate your segments with the separation token tokenizer.sep_token (or
  </s>).
A transformer model replacing the attention matrices by sparse matrices to go f