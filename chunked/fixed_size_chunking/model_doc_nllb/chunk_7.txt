 improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.
This implementation contains the dense models available on release.
The sparse model NLLB-MoE (Mixture of Expert) is now available! More details here
This model was contributed by Lysandre. The authors' code can be found here.
Generating with NLLB
While generating the target text set the forced_bos_token_id to the target language id. The following
example shows ho