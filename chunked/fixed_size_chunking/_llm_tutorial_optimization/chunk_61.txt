se seen during training, c.f Press et al.. However, the community has found a couple of effective tricks that adapt \( \theta \), thereby allowing RoPE position embeddings to work well for extrapolated text input sequences (see here).

Both RoPE and ALiBi are relative positional embeddings that are not learned during training, but instead are based on the following intuitions:
 -   Positional cues about the text inputs should be given directly to the \( QK^T \) matrix of the self-attention layer
 -   The LL