 can also use an utility method called release_memory()
thon
from accelerate.utils import release_memory

release_memory(model)

Now what if your GPU does not have 32 GB of VRAM? It has been found that model weights can be quantized to 8-bit or 4-bits without a significant loss in performance (see Dettmers et al.).
Model can be quantized to even 3 or 2 bits with an acceptable loss in performance as shown in the recent GPTQ paper ðŸ¤¯.
Without going into too many details, quantization schemes aim at reducing th