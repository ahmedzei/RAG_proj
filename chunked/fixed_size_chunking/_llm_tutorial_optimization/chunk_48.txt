 passing a short input sequence as done in the beginning.
py
flush()
For more information on how to use Flash Attention, please have a look at this doc page.
3. Architectural Innovations
So far we have looked into improving computational and memory efficiency by:

Casting the weights to a lower precision format
Replacing the self-attention algorithm with a more memory- and compute efficient version

Let's now look into how we can change the architecture of an LLM so that it is most effective and efficient f