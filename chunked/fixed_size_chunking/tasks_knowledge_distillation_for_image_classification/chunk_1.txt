puts and the teacher's outputs, thus making it mimic the behavior. It was first introduced in Distilling the Knowledge in a Neural Network by Hinton et al. In this guide, we will do task-specific knowledge distillation. We will use the beans dataset for this.
This guide demonstrates how you can distill a fine-tuned ViT model (teacher model) to a MobileNet (student model) using the TrainerÂ API of ðŸ¤— Transformers. 
Let's install the libraries needed for distillation and evaluating the process. 

pip install tr