s of LLM prompting
In this section of the guide we have compiled a list of best practices that tend to improve the prompt results:

When choosing the model to work with, the latest and most capable models are likely to perform better. 
Start with a simple and short prompt, and iterate from there.
Put the instructions at the beginning of the prompt, or at the very end. When working with large context, models apply various optimizations to prevent Attention complexity from scaling quadratically. This may make