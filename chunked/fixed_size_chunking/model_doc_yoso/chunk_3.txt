ined Transformer. On the Long Range Arena (LRA) benchmark, 
for evaluating performance on long sequences, our method achieves results consistent with softmax self-attention but with sizable 
speed-ups and memory savings and often outperforms other efficient self-attention methods. Our code is available at this https URL
This model was contributed by novice03. The original code can be found here.
Usage tips

The YOSO attention algorithm is implemented through custom CUDA kernels, functions written in CUDA C+