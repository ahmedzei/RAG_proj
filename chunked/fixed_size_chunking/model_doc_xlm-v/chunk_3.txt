ith XLM-V on downstream tasks.
The experiments repository can be found here.
Usage tips

XLM-V is compatible with the XLM-RoBERTa model architecture, only model weights from fairseq
  library had to be converted.
The XLMTokenizer implementation is used to load the vocab and performs tokenization.

A XLM-V (base size) model is available under the facebook/xlm-v-base identifier.

XLM-V architecture is the same as XLM-RoBERTa, refer to XLM-RoBERTa documentation for API reference, and examples.
