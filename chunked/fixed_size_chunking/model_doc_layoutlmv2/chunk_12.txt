hidden states of the model will have a
  length of 512 + 49 = 561, if you pad the text tokens up to the max length. More generally, the last hidden states
  will have a shape of seq_length + image_feature_pool_shape[0] *
  config.image_feature_pool_shape[1].
When calling [~transformers.LayoutLMv2Model.from_pretrained], a warning will be printed with a long list of
  parameter names that are not initialized. This is not a problem, as these parameters are batch normalization
  statistics, which are going to h