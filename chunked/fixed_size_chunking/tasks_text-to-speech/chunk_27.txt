put_ids=input_ids, labels=label_features, return_tensors="pt")
         # replace padding with -100 to ignore loss correctly
         batch["labels"] = batch["labels"].masked_fill(batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100)
         # not used during fine-tuning
         del batch["decoder_attention_mask"]
         # round down target lengths to multiple of reduction factor
         if model.config.reduction_factor > 1:
             target_lengths = torch.tensor([len(feature["input_values"]) for