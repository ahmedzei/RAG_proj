 been trained/fine-tuned, it can be saved/loaded just like
any other models (see the examples for more information).
An application of this architecture could be to leverage two pretrained [BertModel] as the encoder
and decoder for a summarization model as was shown in: Text Summarization with Pretrained Encoders by Yang Liu and Mirella Lapata.
Randomly initializing EncoderDecoderModel from model configurations.
[EncoderDecoderModel] can be randomly initialized from an encoder and a decoder config. In the f