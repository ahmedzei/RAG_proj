ration process
model.cuda()
Keep running the quantized model

Export to ONNX
The goal of exporting to ONNX is to deploy inference by TensorRT. Fake
quantization will be broken into a pair of QuantizeLinear/DequantizeLinear ONNX ops. After setting static member of
TensorQuantizer to use Pytorchâ€™s own fake quantization functions, fake quantized model can be exported to ONNX, follow
the instructions in torch.onnx. Example:
thon

from pytorch_quantization.nn import TensorQuantizer
TensorQuantizer.use_fb_fake_qu