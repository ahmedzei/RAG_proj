etter Transformer support covered above.
Next, install the latest version of Flash Attention 2:

pip install -U flash-attn --no-build-isolation
Usage
To load a model using Flash Attention 2, we can pass the attn_implementation="flash_attention_2" flag to .from_pretrained. We'll also load the model in half-precision (e.g. torch.float16), since it results in almost no degradation to audio quality but significantly lower memory usage and faster inference:
python
model = BarkModel.from_pretrained("suno/bark-sma