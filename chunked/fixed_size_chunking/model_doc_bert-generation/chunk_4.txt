r.from_pretrained("google-bert/bert-large-uncased")
input_ids = tokenizer(
     "This is a long article to summarize", add_special_tokens=False, return_tensors="pt"
 ).input_ids
labels = tokenizer("This is a short summary", return_tensors="pt").input_ids
train
loss = bert2bert(input_ids=input_ids, decoder_input_ids=labels, labels=labels).loss
loss.backward()

Pretrained [EncoderDecoderModel] are also directly available in the model hub, e.g.:
thon

instantiate sentence fusion model
sentence_fuser = EncoderD