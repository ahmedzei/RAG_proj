LLMs for speed and memory;
Guide on quantization such as bitsandbytes and autogptq, which shows you how to drastically reduce your memory requirements.

Related libraries

text-generation-inference, a production-ready server for LLMs;
optimum, an extension of ðŸ¤— Transformers that optimizes for specific hardware devices.
