l with absolute position embeddings so it's usually advised to pad the inputs on the right rather than
  the left.
The model architecture is based on the original Transformer, but replaces the “standard” self-attention mechanism in the decoder by a a main self-attention mechanism and a self and n-stream (predict) self-attention mechanism.

Resources

Causal language modeling task guide
Translation task guide
Summarization task guide

ProphetNetConfig
[[autodoc]] ProphetNetConfig
ProphetNetTokenizer
[[autodo