Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.
The Authors' code can be found here.
Resources

Causal language modeling task guide
Translation task guide
Summarization task guide

XLMProphetNetConfig
[[autodoc]] XLMProphetNetConfig
XLMProphetNetTokeni