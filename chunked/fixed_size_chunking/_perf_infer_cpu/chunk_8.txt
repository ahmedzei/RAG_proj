 only provides a brief and simple example.

ONNX Runtime (ORT) is a model accelerator that runs inference on CPUs by default. ORT is supported by ðŸ¤— Optimum which can be used in ðŸ¤— Transformers, without making too many changes to your code. You only need to replace the ðŸ¤— Transformers AutoClass with its equivalent [~optimum.onnxruntime.ORTModel] for the task you're solving, and load a checkpoint in the ONNX format.
For example, if you're running inference on a question answering task, load the optimum/roberta-