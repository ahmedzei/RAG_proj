uery Attention.
Implementation details
The main differences compared to GPT2.
- Added support for Multi-Query Attention.
- Use gelu_pytorch_tanh instead of classic gelu.
- Avoid unnecessary synchronizations (this has since been added to GPT2 in #20061, but wasn't in the reference codebase).
- Use Linear layers instead of Conv1D (good speedup but makes the checkpoints incompatible).
- Merge _attn and _upcast_and_reordered_attn. Always merge the matmul with scaling. Rename reorder_and_upcast_attn->attention_s