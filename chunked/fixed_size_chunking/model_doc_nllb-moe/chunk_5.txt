 on the implementation of SwitchTransformers.
The tokenizer is the same as the NLLB models.

Implementation differences with SwitchTransformers
The biggest difference is the way the tokens are routed. NLLB-MoE uses a top-2-gate which means that for each input, only the top two experts are selected based on the 
highest predicted probabilities from the gating network, and the remaining experts are ignored. In SwitchTransformers, only the top-1 probabilities are computed, 
which means that tokens have less pr