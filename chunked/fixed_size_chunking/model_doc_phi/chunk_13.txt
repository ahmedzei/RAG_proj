toTokenizer.from_pretrained("microsoft/phi-1_5")
feel free to change the prompt to your liking.
prompt = "If I were an AI that had just achieved"
apply the tokenizer.
tokens = tokenizer(prompt, return_tensors="pt").to("cuda")
use the model to generate new tokens.
generated_output = model.generate(**tokens, use_cache=True, max_new_tokens=10)
tokenizer.batch_decode(generated_output)[0]
'If I were an AI that had just achieved a breakthrough in machine learning, I would be thrilled'

Expected speedups
Below is 