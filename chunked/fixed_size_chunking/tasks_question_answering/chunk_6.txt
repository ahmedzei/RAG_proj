ss_function(examples):
     questions = [q.strip() for q in examples["question"]]
     inputs = tokenizer(
         questions,
         examples["context"],
         max_length=384,
         truncation="only_second",
         return_offsets_mapping=True,
         padding="max_length",
     )

     offset_mapping = inputs.pop("offset_mapping")
     answers = examples["answers"]
     start_positions = []
     end_positions = []
     for i, offset in enumerate(offset_mapping):
         answer = answers[i]
    