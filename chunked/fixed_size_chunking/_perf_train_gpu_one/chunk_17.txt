 
(such as the Ampere architecture) offer bf16 and tf32 (CUDA internal data type) data types. Check 
out the NVIDIA Blog to learn more about 
the differences between these data types.
fp16
The main advantage of mixed precision training comes from saving the activations in half precision (fp16). 
Although the gradients are also computed in half precision they are converted back to full precision for the optimization 
step so no memory is saved here. 
While mixed precision training results in faster computati