becomes 4. 
Alternatively, use ðŸ¤— Accelerate to gain full control over the training loop. Find the ðŸ¤— Accelerate example 
further down in this guide.
While it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can 
result in a more pronounced training slowdown. Consider the following example. Let's say, the per_device_train_batch_size=4 
without gradient accumulation hits the GPU's limit. If you would like to train with batches of size 64, do not set the 
per_dev