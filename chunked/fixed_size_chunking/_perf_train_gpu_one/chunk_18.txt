ons, it can also lead to more GPU memory being utilized, especially for small batch sizes.
This is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU).
To enable mixed precision training, set the fp16 flag to True:
py
training_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)
If you prefer to use ðŸ¤— Accelerate, find the ðŸ¤— Accelerate example further in this guide. 
BF16
If you have access to an Ampere or newer ha